Docker


What is a docker?
-> Docker is an open-source containerization platform used to build, package, ship, and run applications inside isolated environments called containers.
-> Docker uses OS-level virtualization to run applications in isolated environments without requiring a full virtual machine.
-> Makes deployment and development efficient.



What is a container?
-> A container is a lightweight, executable package that includes an application, its dependencies, runtime, configuration, and system tools.
-> It can be easily shared.
-> Makes deployment and development efficient.
-> Containers share the host OS kernel, which makes them faster, smaller, and more efficient than virtual machines.



Architecture of Docker
1. Docker Client (CLI)
   - Used to execute commands (`docker build`, `docker run`, `docker pull`, etc.)

2. Docker Daemon (dockerd)
   - Responsible for managing containers, images, networks, and volumes.

3. Docker Engine API
   - Communication layer between Docker client and daemon.

4. Docker Registry
   - Stores and distributes images (e.g., Docker Hub, AWS ECR, GCP Artifact Registry).



Container Contains
Application code
Runtime environment (Python/Node/Java etc.)
Dependencies and packages
Configuration and environment variables



Docker Containers
-> Low impact on OS, very fast, low disk space usage
-> Sharing re-building and distribution is easy
-> Encapsulate apps instead of whole machine

VMs
-> High impact in OS, slower, high disk space usage
-> Sparing, re-building and distribution is challenging.
-> Encapsulate whole machine.

| Feature      | Docker Containers            | Virtual Machines                      |
| ------------ | ---------------------------- | ------------------------------------- |
| Kernel       | Share host OS kernel         | Have separate guest OS                |
| Startup Time | Seconds                      | Minutes                               |
| Size         | MBs                          | GBs                                   |
| Performance  | Near-native                  | Slower due to hardware virtualization |
| Use Cases    | Microservices, CI/CD, DevOps | Monolithic apps, Full OS isolation    |

Containers virtualize the OS, whereas VMs virtualize hardware.



Main components of Docker
Dockerfile → Blueprint for creating an image
Docker Image → Read-only package built using Dockerfile
Docker Container → Running instance of the image
Docker Registry → Storage for images (Docker Hub, private registry)

Dockerfile --> docker build --> Image
Image --> docker run --> Container

Docker uses a layered filesystem (UnionFS) to optimize storage and reuse layers across images.



Docker Registry
A Docker registry is a centralized service used to store, manage, and distribute Docker images.
Examples: Docker Hub, GitHub Container Registry, AWS ECR, Google Artifact Registry.



Installing Docker and Configuration
Docker installation involves setting up the Docker Engine, which includes the Docker Daemon (dockerd), Docker CLI, and required runtime components.

After installation, common setup steps are:
sudo systemctl start docker
sudo systemctl enable docker
docker --version
docker info

| Task                     | Purpose                              | Command                         |
| ------------------------ | ------------------------------------ | ------------------------------- |
| Add user to docker group | Run Docker without `sudo`            | `sudo usermod -aG docker $USER` |
| Verify daemon is running | Ensure service health                | `systemctl status docker`       |
| Pull a test image        | Verify network + registry connection | `docker pull hello-world`       |



Working on Docker

A Dockerfile is a text file containing a set of instructions that defines how a Docker image should be built including base image, dependencies, environment variables, working directory, and startup command.

Example structure:
	FROM node:18
	WORKDIR /app
	COPY . .
	RUN npm install
	CMD ["npm", "start"]

Then we build the image:
	docker build -t myapp:latest .

And run a container from that image:
	docker run -d --rm --name "myapp" -p 3000:3000 myapp:latest
		-d: 	to run container in detached mode
		-rm:	to remove the container as and when the container is stopped
		-name:	to name the container according to our will
		-p:	for port binding to user machine

Note:
	docker build → Creates an image from a Dockerfile
	docker run → Starts a container from an image
	docker ps → Shows running containers
	docker stop / rm → Stop & remove containers
	docker images → Lists stored images

Common Runtime Commands
-----------------------

docker exec -it <container> <command>     → Run a command inside a running container
docker logs -f <container>                → View and follow live logs
docker attach <container>                 → Attach to the container’s main process
docker stop <container>                   → Gracefully stop a running container
docker kill <container>                   → Force stop (SIGKILL)
docker restart <container>                → Restart a container
docker rm <container>                     → Remove a stopped container
docker ps -a                              → List all containers (including stopped)




You can run multiple containers from the same Docker image without any conflict because each container runs in its own isolated environment. Each container can map its internal ports to different host ports, which allows multiple instances of the same application to run simultaneously.

Containers can expose the same internal port, but must have unique host port mappings.

Container Lifecycle
-------------------

docker create <image>     → Create a container without starting it
docker start <container>  → Start a previously created container
docker pause <container>  → Pause all processes inside the container
docker unpause <container>→ Resume paused container
docker rm <container>     → Remove a stopped container

Lifecycle Summary:
Create → Start → Run → Stop → Remove


Container isolation is achieved using:
	Namespaces (process, mount, network, user)
	Control Groups (cgroups) for resource limits
	UnionFS layers for filesystem isolation.

Docker allows multiple container instances from the same image because containers share the OS kernel but have isolated userspace environments.



Tagging a image
Docker images are tagged to identify versions and repositories.

Syntax while building:
    docker build -t myapp:01 .

Multiple tags:
    docker build -t myapp:latest -t myapp:01 .

Tag an existing image:
    docker tag myapp:01 myrepo/myapp:01

Pushing Images to Registry
--------------------------

docker login                                 → Authenticate to Docker registry
docker tag myapp:01 username/myapp:01        → Tag image for registry upload
docker push username/myapp:01                → Upload image to registry

Example Upload Flow:
1. docker build -t myapp:01 .
2. docker tag myapp:01 garv127/myapp:01
3. docker push garv127/myapp:01




Benefit of using docker and containers:
Docker allows version control of images, meaning older versions are not lost when a new version is created. You can run multiple versions of the same application simultaneously to test changes without affecting production.


1. Consistency Across Environments
   - "Works on my machine" issue is eliminated since the same image runs everywhere.
2. Portability
   - Containers can run on any environment that supports Docker (Linux, Windows, cloud, on-prem).
3. Faster Deployment
   - Containers start within seconds since they share the host OS kernel.
4. Lightweight
   - Containers consume fewer resources compared to virtual machines.
5. Scalability
   - Containers are easy to scale horizontally using orchestration tools like Docker Swarm or Kubernetes.
6. Version Control of Images
   - You can maintain multiple tagged versions of the same image and run them simultaneously for testing or rollback.
7. Resource Isolation
   - Docker uses namespaces and cgroups to provide process-level isolation and resource limits.
8. CI/CD Friendly
   - Works seamlessly with pipelines and automation tools (Jenkins, GitHub Actions, GitLab CI, ArgoCD).

Docker improves developer productivity, ensures consistent runtime environments, accelerates deployments, and provides versioned and isolated workloads suitable for modern DevOps and microservices architecture.



Docker Image Layers & Caching
-----------------------------

Docker builds images in layers. Each instruction in the Dockerfile
(RUN, COPY, ADD, CMD) creates a separate layer.

If a layer does not change, Docker reuses the cached layer to speed up builds.

Optimized layering example:

COPY requirements.txt .
RUN pip install -r requirements.txt
COPY . .

This ensures dependencies are installed only when requirements.txt changes.




Pre-defined Docker Images:
Docker provides official base images such as node, python, nginx, mysql, and postgres which are maintained and verified by the Docker team or trusted organizations. These images can be used as a starting point to build custom application images.

Types of Docker Images Available:

1. Official Images
   - Verified and maintained by Docker or trusted vendors.
   - Examples: node, python, ubuntu, postgres, nginx.
2. Community Images
   - Created and published by users.
   - May or may not be trusted.
3. Private Images
   - Organization-specific images stored in private registries like AWS ECR, GitHub Container Registry, GCP Artifact Registry, or Harbor.



NGINX
-> NGINX is a high-performance web server that can also act as a reverse proxy, load balancer, and caching server. It is widely used to serve static content and route traffic efficiently for modern web applications.
-> NGINX is a lightweight, fast, and scalable web server originally designed to handle thousands of simultaneous client connections using an asynchronous, event-driven architecture.

In DevOps and cloud environments, NGINX is commonly used as:
| Role          | Purpose                                                                        |
| ------------- | ------------------------------------------------------------------------------ |
| Web Server    | Serves static files (HTML, CSS, JS, images)                                    |
| Reverse Proxy | Forwards client requests to backend applications (Node.js, Python, Java, etc.) |
| Load Balancer | Distributes traffic across multiple backend servers                            |
| Cache Layer   | Stores frequently accessed responses to improve performance                    |
| API Gateway   | Controls, routes, and manages API traffic                                      |

NGINX is an event-driven web server commonly used as a reverse proxy and load balancer because it efficiently handles large amounts of concurrent traffic, making it ideal for microservices and containerized environments.



Creating a interactive Docker Image
-> You can run a Docker container in interactive mode (with terminal access) using the -it flag.
-> This allows you to interact with the container shell as if you were inside a Linux environment.

	docker run -it <image_name_or_id> /bin/bash

| Flag | Meaning                             |
| ---- | ----------------------------------- |
| `-i` | Interactive mode (keeps STDIN open) |
| `-t` | Allocates a pseudo-terminal (TTY)   |


Useful related commands:
    docker exec -it <container_id> /bin/bash   → Open terminal in a running container
    docker start -ai <container_id>            → Start a stopped container interactively
    exit                                       → Leave the container shell



Pulling the images
Pulling an image means downloading it from a container registry (such as Docker Hub) to your local system so it can be used to create containers.
	docker pull <image_name>
If no tag is specified, Docker pulls the default `latest` tag.

Useful related commands:
    docker images            → List downloaded images
    docker search <image>    → Search for images on Docker Hub
    docker rmi <image>       → Remove a local image
    docker inspect <image>   → View image metadata



Docker Volume
-> A Docker volume is a storage mechanism used to persist data generated by containers so that the data remains available even after the container stops, restarts, or is deleted.
-> Docker volumes allow containers to store data outside their writable layer.
-> Since container filesystems are temporary and isolated, any data written inside a container is normally lost when the container is removed. Volumes solve this by providing persistent storage that exists independently of the container lifecycle.

Volumes are used for:
-> Database storage (MySQL, PostgreSQL, MongoDB)
-> Logs and configuration files
-> Shared storage between multiple containers
-> Backups and recovery

Important commands:
	docker volume create mydata			→ Create a volume
	docker run -d -v mydata:/var/lib/mysql MySQL	→ Run a container using volume
	docker volume ls				→ List volume
	docker volume inspect mydata			→ Inspect a volume
	docker volume rm mydata				→ Remove a volume

Volumes are the recommended method for managing persistent data in Docker because they provide better performance, are easy to back up, and are fully managed by Docker.



Bind Mount
A bind mount allows a file or directory from the host machine’s filesystem to be mounted into a container. This means the container directly accesses and uses the host’s file or folder, and any changes made inside the container reflect immediately on the host and vice-versa.

Bind mounts are useful for:
-> Local development (code changes update live inside the running container)
-> Testing configurations without rebuilding images
-> Sharing logs or configuration files between host and container

Unlike Docker-managed volumes, bind mounts are not fully controlled by Docker. They depend on:
-> Host directory path
-> Host system permissions
-> File system structure
-> So they are powerful but can create issues if paths change or permissions are wrong.

docker run -d -p 3000:3000 -v /home/user/app:/usr/src/app node

| Component        | Meaning                    |
| ---------------- | -------------------------- |
| `/home/user/app` | Host directory             |
| `/usr/src/app`   | Directory inside container |
| `-v`             | Mount option               |

Docker Recommended Syntax (Newer Format)
docker run -d \
  --mount type=bind,source=/home/user/app,target=/usr/src/app \
  node

Bind mounts give containers direct access to a host directory, making them ideal for development where live file syncing is needed. However, Docker volumes are preferred for production because they are portable, secure, and fully managed by Docker.



.dockerignore
The .dockerignore file is used to specify files and directories that should be excluded when building a Docker image. It helps prevent unnecessary files from being copied into the image, keeping the image lightweight, secure, and faster to build.

| Benefit                        | Explanation                                                                      |
| ------------------------------ | -------------------------------------------------------------------------------- |
| Smaller image size             | Prevents copying unnecessary files like logs, temp files, source maps, etc.      |
| Faster build time              | Less data to send during `docker build` context transfer.                        |
| Security                       | Prevents sensitive files (like credentials or `.env` files) from being included. |
| Cleaner production environment | Only required application code is packaged.                                      |

How It Works
During the docker build process, Docker sends a copy of the current directory (called the build context) to the Docker daemon.
The .dockerignore file filters out files from this context before copying happens.

The .dockerignore file improves security and performance during Docker builds by excluding unnecessary or sensitive files from the build context.


ENTRYPOINT vs CMD
-----------------

Both are used to define what runs inside a container, but:

ENTRYPOINT → Defines the main executable.
CMD        → Provides default arguments to ENTRYPOINT.

Example:

ENTRYPOINT ["python"]
CMD ["app.py"]

Result: python app.py




Connecting a Container to a Local Database
When working with Docker, applications running inside containers may need to access a database running on the host machine (local system).
Since localhost inside a container refers to the container itself, the host cannot be reached using localhost or 127.0.0.1.

To connect to the local (host) database, use:
| Platform                         | Host Address Inside Container            |
| -------------------------------- | ---------------------------------------- |
| Windows / macOS (Docker Desktop) | `host.docker.internal`                   |
| Linux                            | Host machine IP (typically `172.17.0.1`) |


Cleanup Commands
----------------

docker system prune             → Remove unused data (keeps volumes)
docker system prune -a          → Remove all unused containers, networks, images
docker image prune              → Remove unused images
docker container prune          → Remove stopped containers
docker volume prune             → Remove unused volumes

⚠ WARNING: `-a` removes ALL untagged and unused images — use carefully.




Docker Network
Creating a custom Docker network allows containers to communicate with each other using container names instead of IP addresses. This makes container networking more reliable and easier to manage, especially in multi-container applications.

	docker network create my-net

| Default Bridge Network           | Custom Network                       |
| -------------------------------- | ------------------------------------ |
| Containers communicate using IPs | Containers communicate using names   |
| No automatic DNS                 | Built-in DNS resolution              |
| Not ideal for microservices      | Recommended for multi-container apps |

Useful Network Commands
| Command                                     | Purpose                                        |
| ------------------------------------------- | ---------------------------------------------- |
| `docker network ls`                         | List networks                                  |
| `docker network inspect my-net`             | See network details (IPs, attached containers) |
| `docker network connect my-net <container>` | Attach a container to an existing network      |
| `docker network rm my-net`                  | Remove a network                               |

Creating a custom Docker network enables containers to communicate securely using built-in DNS name resolution. This is essential for microservices, multi-container deployments, and Docker-Compose setups.

Docker Networking
Docker networking enables communication between containers, the host system, and external clients. Networking is essential when building multi-container applications (API + database, frontend + backend, etc.).


Default Docker Network Types
When Docker is installed, it creates three default networks:
| Network Type         | Description                        | Use Case                                   | Communication                                      |
| -------------------- | ---------------------------------- | ------------------------------------------ | -------------------------------------------------- |
| **bridge** (default) | NAT-based private internal network | Local development                          | Containers can communicate only if in same network |
| **host**             | Shares host network stack          | High-performance apps needing no isolation | No port mapping needed                             |
| **none**             | No networking                      | Fully isolated container                   | Completely disconnected                            |


User-Defined Bridge Networks

User-created networks provide:
-> Built-in DNS (containers reach each other by name)
-> Better isolation
-> Custom network control

Create a network:
	docker network create my-net
Run containers inside the network:
	docker run -d --name db --network my-net mysql
	docker run -d --name app --network my-net flask-api
Inside app, the DB is reachable as:
	mysql://db:3306

Exposing vs Publishing Ports
| Type                          | Dockerfile                                | Runtime Accessibility                | Example        |
| ----------------------------- | ----------------------------------------- | ------------------------------------ | -------------- |
| **EXPOSE (internal hint)**    | Used only as documentation                | Does *not* open port to host         | `EXPOSE 5000`  |
| **Publish (-p or --publish)** | Makes container port available externally | Required to access from host/browser | `-p 8080:5000` |


docker run -d -p 8080:5000 myapp
| Value  | Meaning               |
| ------ | --------------------- |
| `5000` | Port inside container |
| `8080` | Port on host machine  |

Docker Compose Networking
Docker Compose automatically creates a network for all services, and containers communicate using service names as hostnames.

Example docker-compose.yml:

version: "3.9"

services:
  db:
    image: postgres
    environment:
      POSTGRES_PASSWORD: admin

  app:
    build: .
    depends_on:
      - db
    ports:
      - "5000:5000"

-> The service name db becomes the hostname.
-> No need to manually define or connect networks unless custom behavior is needed.

In Docker, communication between containers is best handled using user-defined networks where services discover each other by name. External access requires port publishing, whereas internal exposure uses EXPOSE. Docker Compose simplifies this by creating a default network where all services communicate securely.

Docker Network Modes Summary
----------------------------

| Network Mode | Behavior                       | Use Case                          |
|--------------|--------------------------------|----------------------------------|
| bridge       | Default isolated network        | Most standalone containers       |
| host         | Shares host networking stack    | High performance apps            |
| none         | No networking                   | Secure isolated workloads        |
| custom       | User-defined with DNS          | Microservices & multi-container apps |

Custom networks support automatic hostname resolution,
so containers communicate using names, not IP addresses.



Docker Compose
Docker Compose is a tool that allows you to define and manage multi-container applications using a configuration file (docker-compose.yml). It automates container creation, networking, environment variables, storage, and startup order — all from a single command.

| Without Compose                     | With Docker Compose         |
| ----------------------------------- | --------------------------- |
| Need multiple `docker run` commands | One file defines everything |
| Manual networking setup             | Automatic shared network    |
| Hard to maintain                    | Version-controlled config   |
| Difficult to manage scaling         | Built-in scaling support    |

In real projects, you may need:
-> API container
-> Database container
-> Cache container
-> Web server container
Compose manages them together as one application.

| Command                  | Purpose                             |
| ------------------------ | ----------------------------------- |
| `docker compose up`      | Start containers                    |
| `docker compose up -d`   | Start in background (detached mode) |
| `docker compose down`    | Stop and remove containers, network |
| `docker compose ps`      | List running services               |
| `docker compose logs -f` | View live logs                      |

Networking in Compose
-> All services in the same Compose file join the same automatic network.
-> Containers communicate using service names, not IPs.

Example:
	app container can connect to PostgreSQL using hostname: database

Scaling in Docker Compose
-------------------------

docker compose up --scale <service>=<number>

Example:
docker compose up --scale app=3

This runs 3 replicas of the `app` service (stateless recommended).


Docker Compose is used to define, run, and manage multi-container applications using a YAML file. It ensures containers start in the correct order, share networking automatically, and can be easily scaled or version-controlled.

Security Best Practices
-----------------------

- Use minimal base images (alpine, distroless) to reduce attack surface
- Avoid running processes as root:
    USER nonroot
- Do not store secrets inside images or commit .env files
- Use .dockerignore to prevent accidental secret leakage
- Scan images for vulnerabilities (Docker Scout, Trivy)
